{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "[![arXiv:2409.15370](https://img.shields.io/badge/cs.LG-2409.15370-b31b1b?style=flat&amp;logo=arxiv&amp;logoColor=red)](https://arxiv.org/abs/2409.15370)\n",
    "![PyPI - Downloads](https://img.shields.io/pypi/dm/smirk)\n",
    "[![GitHub Release Date](https://img.shields.io/github/release-date/BattModels/smirk?display_date=published_at&logo=github)\n",
    "](https://github.com/BattModels/smirk)\n",
    "\n",
    "\n",
    "Molecular Foundation Models are all the rage, but without a tokenizer that can represent *all* of chemistry, any model will be inheriently limited. Current \"atomwise\" tokenizers are fundementally limited by requiring a different token for every \"bracketed atom\" in the [SMILES](https://en.wikipedia.org/wiki/Simplified_Molecular_Input_Line_Entry_System) encoding for a compound.\n",
    "\n",
    "The problem is, most atoms are bracketed. Most elements (i.e. lithium `[Li]`), chiral centers, isotops, any charged species all require bracketed atoms. Compounds where bracketed atoms are critical to their effectiveness include:\n",
    "\n",
    "- [Cisplatin](https://en.wikipedia.org/wiki/Cisplatin): Effective Chemotherapy drug, but it's isomer ([transplatin](https://en.wikipedia.org/wiki/Transplatin)) is not.\n",
    "- [Sodium pertechnetate](https://en.wikipedia.org/wiki/Sodium_pertechnetate): [Radiopharmaceutical](https://en.wikipedia.org/wiki/Radiopharmaceutical) used for thyroid imaging\n",
    "- [Lithium Iron Phosphate](https://en.wikipedia.org/wiki/Lithium_iron_phosphate): Widely used cathode material within EV battery packs.\n",
    "\n",
    "Smirk fixes this by fully tokenizing a SMILES string all the way down to it's consitent elements. Enabling a vocab only 167 tokens to represent all of [OpenSMILES](http://opensmiles.org/) with all the special tokens needed for language modeling baked in.\n",
    "\n",
    "Check out the paper for all the details, but otherwise let's see it in action ðŸ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "ðŸ Installation is easy with pre-build binaries on [PyPI](https://pypi.org/project/smirk/) and [GitHub](https://github.com/BattModels/smirk/releases). Just run: `pip install smirk`\n",
    "> Want to install from source? See [installing from source](./developer.md#installing-from-source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "!python -m pip install smirk transformers rdkit torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "ðŸ¤— smirk is built using HuggingFace's [tokenizers](https://huggingface.co/docs/tokenizers) library for out-of-the box compatability with [transformers's PreTrainedTokenizerFast](https://huggingface.co/docs/transformers/main_classes/tokenizer). No need to learn another tokenizer, things just work out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smirk import SmirkTokenizerFast\n",
    "\n",
    "# Just import and tokenize!\n",
    "smirk = SmirkTokenizerFast()\n",
    "smirk(\"CC(=O)Nc1ccc(O)cc1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Tokenization with Padding\n",
    "batch = smirk([\n",
    "    \"C[C@@H]1CCCCCCCCCCCCC(=O)C1\",\n",
    "    \"O=C(O)C[C@H](N)C(=O)N[C@H](C(=O)OC)Cc1ccccc1\",\n",
    "    \"CN(C)S[N][Re@OH18]([C][O])([C][O])([C][O])([C][O])[C][O]\"\n",
    "], padding=\"longest\")\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to molecules!\n",
    "smirk.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, we don't add `[CLS]` and `[SEP]` tokens, but that's just a flag\n",
    "smirk_bert = SmirkTokenizerFast(template=\"[CLS] $0 [SEP]\")\n",
    "\" \".join(smirk_bert.tokenize(\"CNCCC(c1ccccc1)Oc2ccc(cc2)C(F)(F)F\", add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## What Makes Smirk Special?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizers = {\n",
    "    \"smirk\": smirk,\n",
    "    \"molformer\": AutoTokenizer.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", trust_remote_code=True),\n",
    "    \"GPT-4o\": AutoTokenizer.from_pretrained(\"Xenova/gpt-4o\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import MolsToGridImage, rdMolDraw2D\n",
    "from IPython.display import SVG\n",
    "\n",
    "smi = [\n",
    "    \"Cl[Pt@SP1](Cl)([NH3])[NH3]\", # Cisplatin \n",
    "    \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\", # Caffine\n",
    "    \"NCCc1cc(O)c(O)cc1\", # Dopamine\n",
    "    \"Cl[Pt@SP2](Cl)([NH3])[NH3]\", # Transplatin\n",
    "    \"[NH4+].[NH4+].OP([O-])([O-])=O\", # Diammonium phosphate\n",
    "    \"[O-][99Tc](=O)(=O)=O.[Na+]\", # Sodium pertechnetate with radiotracer\n",
    "\n",
    "]\n",
    "#smi = [smi[0]]\n",
    "def get_legend(smi:str, tokenizers:dict):\n",
    "    entries = []\n",
    "    for name, tok in tokenizers.items():\n",
    "        entries.append(f\"{name}: {' '.join(tok.tokenize(smi))}\")\n",
    "    return \"\\n\".join(entries)\n",
    "\n",
    "\n",
    "drawOptions = rdMolDraw2D.MolDrawOptions()\n",
    "drawOptions.fixedScale = 1\n",
    "drawOptions.centreMoleculesBeforeDrawing = True\n",
    "drawOptions.minFontSize = 6\n",
    "drawOptions.legendFontSize = 24\n",
    "drawOptions.legendFraction = 0.3\n",
    "MolsToGridImage(\n",
    "    [Chem.MolFromSmiles(smi) for smi in smi],\n",
    "    molsPerRow=2, subImgSize=(400,200),\n",
    "    legends=[get_legend(smi, tokenizers) for smi in smi],\n",
    "    drawOptions=drawOptions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Zero to Molecular Foundation Model with Smirk!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### HuggingFace Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "!python -m pip install accelerate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": [
     "remove-stderr"
    ]
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import Trainer, TrainingArguments, RobertaForMaskedLM, RobertaConfig, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "\n",
    "# MoleculeNet's QM9 dataset. Normally this would be a larger (and unlabeled)\n",
    "# dataset. But for a demo, it's perfect\n",
    "dataset = load_dataset(\"csv\", \n",
    "    data_files=[\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/qm9.csv\"],\n",
    ")[\"train\"].select_columns(\"smiles\").train_test_split(test_size=0.2)\n",
    "\n",
    "# Tokenizer the splits! For a larger dataset, this would be done on-the-fly\n",
    "dataset = dataset.map(smirk, input_columns=[\"smiles\"], desc=\"Tokenizing\", num_proc=1)\n",
    "\n",
    "# A very small model\n",
    "config = RobertaConfig(\n",
    "    vocab_size=len(smirk),\n",
    "    hidden_size=256,\n",
    "    intermediate_size=1024,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=4,\n",
    ")\n",
    "model = RobertaForMaskedLM(config)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    processing_class=smirk,\n",
    "    data_collator=DataCollatorForLanguageModeling(smirk),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
